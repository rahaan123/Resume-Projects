{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rma-job8Rb_"
      },
      "source": [
        "# CSCE 636 Project 2\n",
        "## Rahaan Gandhi - 434007427\n",
        "\n",
        "Please checkout Instructions at bottom of notebook for instructions on how to load and use a saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QFkjHZNAlaRT"
      },
      "outputs": [],
      "source": [
        "# Import all important libraries required for the transformer model\n",
        "# Note for TA/ Professor: run this cell\n",
        "import warnings\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eqiuKxs3mLNX"
      },
      "outputs": [],
      "source": [
        "# Below is the file path for training input and training output files, This data will be used to train for translation\n",
        "# Please replace below paths with correct file paths for the same files on your device\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "input_texts_path = \"/content/drive/MyDrive/Colab Notebooks/636 stuff/project2/Train_input\"\n",
        "output_texts_path = \"/content/drive/MyDrive/Colab Notebooks/636 stuff/project2/Train_output\"\n",
        "train_inputs=pickle.load(open(input_texts_path, 'rb'))\n",
        "train_outputs=pickle.load(open(output_texts_path, 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TZQtOOZ0cXV"
      },
      "outputs": [],
      "source": [
        "# We split dataset to obtain a list of 1000 strings approx, on which we can test our model for its actual performance acc.\n",
        "# Note for TA/ Professor: dont need to run this cell, this cell is only to divide data into train/test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(train_inputs, train_outputs, test_size=0.009)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVWEXkYJmgFe",
        "outputId": "bcc5b8f6-4dbb-4e7e-dbd0-166fa11d579a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "18\n"
          ]
        }
      ],
      "source": [
        "# Here we are setting up the vocab for both input language and output language\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "input_vocab = set()\n",
        "for i in range(len(train_inputs)):\n",
        "  for j in train_inputs[i].split():\n",
        "    input_vocab.add(j)\n",
        "\n",
        "print(len(input_vocab))\n",
        "\n",
        "output_vocab=set()\n",
        "for i in range(len(train_outputs)):\n",
        "  for j in train_outputs[i].split():\n",
        "    output_vocab.add(j)\n",
        "\n",
        "print(len(output_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3qawcpfPmnY9"
      },
      "outputs": [],
      "source": [
        "# Here we split the dataset in train and validation dataset after combining input and output language as a pair\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "sentence_pair=[]\n",
        "for i in range(len(train_inputs)):\n",
        "  input = train_inputs[i]\n",
        "  output = \"[start] \" + train_outputs[i] + \" [end]\"\n",
        "  sentence_pair.append((input,output))\n",
        "\n",
        "random.shuffle(sentence_pair)\n",
        "num_val = int(0.15 * len(sentence_pair))\n",
        "num_train = len(sentence_pair) - 2 * num_val\n",
        "train_sentences = sentence_pair[:num_train]\n",
        "validation_sentences = sentence_pair[num_train:num_train + num_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P8PQaa6Imr3T"
      },
      "outputs": [],
      "source": [
        "# Below defined code and functions are important since they are used to pre-process data\n",
        "# so that it can be vectorized and fed to the model for better translation\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "vocab_size = 35 #since total vocabulary size is 34\n",
        "sequence_length = 100 #since maximum length currently for output is 95\n",
        "\n",
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def sentence_vectorizer(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=sentence_vectorizer,\n",
        ")\n",
        "\n",
        "train_input_texts = [pair[0] for pair in train_sentences]\n",
        "train_output_texts = [pair[1] for pair in train_sentences]\n",
        "source_vectorization.adapt(train_input_texts)\n",
        "target_vectorization.adapt(train_output_texts)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def format_dataset(inp, out):\n",
        "    inp = source_vectorization(inp)\n",
        "    out = target_vectorization(out)\n",
        "    return ({\n",
        "        \"input\": inp,\n",
        "        \"output\": out[:, :-1],\n",
        "    }, out[:,1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    input_texts, output_texts = zip(*pairs)\n",
        "    input_texts = list(input_texts)\n",
        "    output_texts = list(output_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_texts, output_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "Train = make_dataset(train_sentences)\n",
        "validate = make_dataset(validation_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DDkqhAsUnsMX"
      },
      "outputs": [],
      "source": [
        "# Definition of transformer's encoder layer function\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, hidden_layer, dropout_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(hidden_layer, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.normalization_layer_1 = layers.LayerNormalization()\n",
        "        self.normalization_layer_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        att_out = self.att(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.normalization_layer_1(inputs + att_out)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.normalization_layer_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"hidden_layer\": self.hidden_layer,\n",
        "            \"dropout_prob\": self.dropout_prob,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CobsHj7Vp782"
      },
      "outputs": [],
      "source": [
        "# Definition of transformer's decoder layer function\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, hidden_layer, dropout_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.att_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.att_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(hidden_layer, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.normalization_layer_1 = layers.LayerNormalization()\n",
        "        self.normalization_layer_2 = layers.LayerNormalization()\n",
        "        self.normalization_layer_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"hidden_layer\": self.hidden_layer,\n",
        "            \"dropout_prob\": self.dropout_prob,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_att_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_att_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        att_out_1 = self.att_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        att_out_1 = self.normalization_layer_1(inputs + att_out_1)\n",
        "        att_out_2 = self.att_2(\n",
        "            query=att_out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        att_out_2 = self.normalization_layer_2(\n",
        "            att_out_1 + att_out_2)\n",
        "        proj_output = self.dense_proj(att_out_2)\n",
        "        return self.normalization_layer_3(att_out_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a67jS71VnwLK"
      },
      "outputs": [],
      "source": [
        "# Defining the Positional Embedder layer function\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell has configurations for each layer, model build by layer and compile and fit function calls\n",
        "# Configurations for layers\n",
        "# Note for TA/ Professor: run this cell\n",
        "embed_size = 256\n",
        "dense_size = 2048\n",
        "num_heads = 8\n",
        "hidden_layer = 512\n",
        "dropout_prob = 0.5"
      ],
      "metadata": {
        "id": "mJwLvrMKrjvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer's layers\n",
        "# Note for TA/ Professor: run this cell only if you want to build a new model\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_size)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_size, dense_size, num_heads, hidden_layer, dropout_prob)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_size)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_size, dense_size, num_heads, hidden_layer, dropout_prob)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "ev9FJ9wercbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgAdlATQnx3Q",
        "outputId": "3e9cada4-7c75-40ae-d696-a1219af717fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2428/2428 [==============================] - 348s 139ms/step - loss: 0.7322 - accuracy: 0.7142 - val_loss: 0.4274 - val_accuracy: 0.8153\n",
            "Epoch 2/20\n",
            "2428/2428 [==============================] - 334s 137ms/step - loss: 0.4118 - accuracy: 0.8243 - val_loss: 0.3537 - val_accuracy: 0.8424\n",
            "Epoch 3/20\n",
            "2428/2428 [==============================] - 332s 137ms/step - loss: 0.3253 - accuracy: 0.8592 - val_loss: 0.2553 - val_accuracy: 0.8886\n",
            "Epoch 4/20\n",
            "2428/2428 [==============================] - 333s 137ms/step - loss: 0.2219 - accuracy: 0.9065 - val_loss: 0.1435 - val_accuracy: 0.9379\n",
            "Epoch 5/20\n",
            "2428/2428 [==============================] - 334s 138ms/step - loss: 0.1448 - accuracy: 0.9409 - val_loss: 0.0869 - val_accuracy: 0.9637\n",
            "Epoch 6/20\n",
            "2428/2428 [==============================] - 333s 137ms/step - loss: 0.0994 - accuracy: 0.9606 - val_loss: 0.0579 - val_accuracy: 0.9759\n",
            "Epoch 7/20\n",
            "2428/2428 [==============================] - 331s 137ms/step - loss: 0.0729 - accuracy: 0.9719 - val_loss: 0.0442 - val_accuracy: 0.9822\n",
            "Epoch 8/20\n",
            "2428/2428 [==============================] - 332s 137ms/step - loss: 0.0568 - accuracy: 0.9784 - val_loss: 0.0779 - val_accuracy: 0.9697\n",
            "Epoch 9/20\n",
            "2428/2428 [==============================] - 332s 137ms/step - loss: 0.0464 - accuracy: 0.9829 - val_loss: 0.0258 - val_accuracy: 0.9900\n",
            "Epoch 10/20\n",
            "2428/2428 [==============================] - 331s 136ms/step - loss: 0.0396 - accuracy: 0.9856 - val_loss: 0.0469 - val_accuracy: 0.9855\n",
            "Epoch 11/20\n",
            "2428/2428 [==============================] - 331s 136ms/step - loss: 0.0343 - accuracy: 0.9876 - val_loss: 0.0287 - val_accuracy: 0.9891\n",
            "Epoch 12/20\n",
            "2428/2428 [==============================] - 347s 143ms/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 0.0157 - val_accuracy: 0.9940\n",
            "Epoch 13/20\n",
            "2428/2428 [==============================] - 331s 136ms/step - loss: 0.0274 - accuracy: 0.9903 - val_loss: 0.0378 - val_accuracy: 0.9888\n",
            "Epoch 14/20\n",
            "2428/2428 [==============================] - 331s 136ms/step - loss: 0.0243 - accuracy: 0.9915 - val_loss: 0.0137 - val_accuracy: 0.9946\n",
            "Epoch 15/20\n",
            "2428/2428 [==============================] - 348s 143ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0155 - val_accuracy: 0.9943\n",
            "Epoch 16/20\n",
            "2428/2428 [==============================] - 330s 136ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0116 - val_accuracy: 0.9957\n",
            "Epoch 17/20\n",
            "2428/2428 [==============================] - 347s 143ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.0079 - val_accuracy: 0.9970\n",
            "Epoch 18/20\n",
            "2428/2428 [==============================] - 331s 136ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.0078 - val_accuracy: 0.9971\n",
            "Epoch 19/20\n",
            "2428/2428 [==============================] - 330s 136ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0062 - val_accuracy: 0.9977\n",
            "Epoch 20/20\n",
            "2428/2428 [==============================] - 347s 143ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0053 - val_accuracy: 0.9981\n"
          ]
        }
      ],
      "source": [
        "# Note for TA/ Professor: run this cell only if you want to build a new model\n",
        "\n",
        "transformer.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(Train, epochs=20, validation_data=validate)\n",
        "transformer.save(\"transformer0.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdvNYQ5KqtPM"
      },
      "outputs": [],
      "source": [
        "# Load the model using custom_object_scope\n",
        "# If model created in this running instance use below code\n",
        "# Note for TA/ Professor: run this cell if building a new model using above code\n",
        "\n",
        "from keras.utils import custom_object_scope\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Define the custom objects dictionary\n",
        "custom_objects = {'PositionalEmbedding': PositionalEmbedding, 'TransformerEncoder': TransformerEncoder, 'TransformerDecoder': TransformerDecoder}\n",
        "\n",
        "with custom_object_scope(custom_objects):\n",
        "    loaded_model = transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "igzVOgYdqtPM"
      },
      "outputs": [],
      "source": [
        "# If a saved model has to be loaded then use the below lines\n",
        "# Note for TA/ Professor: run this cell if loading a model for testing\n",
        "\n",
        "from keras.utils import custom_object_scope\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Define the custom objects dictionary\n",
        "custom_objects = {'PositionalEmbedding': PositionalEmbedding, 'TransformerEncoder': TransformerEncoder, 'TransformerDecoder': TransformerDecoder}\n",
        "\n",
        "with custom_object_scope(custom_objects):\n",
        "    model_path = \"/content/drive/MyDrive/Colab Notebooks/636 stuff/project2/transformer0.h5\" # path to model\n",
        "    loaded_model = models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bDj90SnFqEw9"
      },
      "outputs": [],
      "source": [
        "# Phase 2 of program where we obtain translated vectorized output from our custom transformer model\n",
        "# We feed that output data into below defined function to decode the vectorized data to readable data\n",
        "# Note for TA/ Professor: run this cell\n",
        "\n",
        "\n",
        "target_vocab = target_vectorization.get_vocabulary()\n",
        "target_index_lookup = dict(zip(range(len(target_vocab)), target_vocab))\n",
        "max_translated_sentence_length = 100\n",
        "\n",
        "def translate(input_sentence):\n",
        "    vect_input_sentence = source_vectorization([input_sentence])\n",
        "    translated_sentence = \"[start]\"\n",
        "    for i in range(max_translated_sentence_length):\n",
        "        vect_target_sentence = target_vectorization(\n",
        "            [translated_sentence])[:, :-1]\n",
        "        pred = loaded_model(\n",
        "            [vect_input_sentence, vect_target_sentence])\n",
        "        sampled_token_index = np.argmax(pred[0, i, :])\n",
        "        sampled_token = target_index_lookup[sampled_token_index]\n",
        "        translated_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"test file path here\"\n",
        "my_test_input = pickle.load(open(test_path, 'rb'))\n",
        "output_list = []\n",
        "for i in range(len(my_test_input)):\n",
        "  input_sentence = my_test_input[i]\n",
        "  output = translate(input_sentence)\n",
        "  output = output[8:-6]\n",
        "  output_list.append(output)\n",
        "\n",
        "output_file_path = \"Rahaan_Gandhi_434007427_Project2_Prediction\"\n",
        "with open(output_file_path, \"wb\") as path:\n",
        "  pickle.dump(output_list, path)"
      ],
      "metadata": {
        "id": "P0wUYqn7vrrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a custom defined function thats used to calculate accuracy of model by feeding it 1000 strings from an independent\n",
        "# test set and then comparing the given output against ideal translation to get actual testing accuracy of the model.\n",
        "''' acc = 0\n",
        "realdata = test_outputs # replace with test_outputs\n",
        "output_list = []\n",
        "my_test_input = test_inputs # replace with test_inputs\n",
        "for i in range(len(my_test_input)):\n",
        "  input_sentence = my_test_input[i]\n",
        "  output = translate(input_sentence)\n",
        "  output = output[8:-6]\n",
        "  output_list.append(output)\n",
        "  for j in range(len(output)):\n",
        "    if output[j] != realdata[i][j]:\n",
        "      print(f\"\\nfor iteration {i}:\")\n",
        "      print(\"actual output:\", output)\n",
        "      print(\"ideal output: \", realdata[i])\n",
        "      flag = False\n",
        "      break\n",
        "    else:\n",
        "      flag = True\n",
        "\n",
        "  if flag == True:\n",
        "    print(f\"\\nfor iteration {i}: match!\")\n",
        "    acc += 1\n",
        "\n",
        "acc = (acc/len(my_test_input)) * 100.00\n",
        "print(f\"acc = {acc}%\")'''"
      ],
      "metadata": {
        "id": "3QXqJYHsFxL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions:\n",
        "Run the below Cell, to load model for testing purposes"
      ],
      "metadata": {
        "id": "PJy5BHTUsw0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all important libraries required for the transformer model\n",
        "# Note for TA/ Professor: run this cell\n",
        "import warnings\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, hidden_layer, dropout_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(hidden_layer, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.normalization_layer_1 = layers.LayerNormalization()\n",
        "        self.normalization_layer_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        att_out = self.att(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.normalization_layer_1(inputs + att_out)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.normalization_layer_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"hidden_layer\": self.hidden_layer,\n",
        "            \"dropout_prob\": self.dropout_prob,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, hidden_layer, dropout_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.att_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.att_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dropout(dropout_prob),\n",
        "             layers.Dense(hidden_layer, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.normalization_layer_1 = layers.LayerNormalization()\n",
        "        self.normalization_layer_2 = layers.LayerNormalization()\n",
        "        self.normalization_layer_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"hidden_layer\": self.hidden_layer,\n",
        "            \"dropout_prob\": self.dropout_prob,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_att_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_att_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        att_out_1 = self.att_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        att_out_1 = self.normalization_layer_1(inputs + att_out_1)\n",
        "        att_out_2 = self.att_2(\n",
        "            query=att_out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        att_out_2 = self.normalization_layer_2(\n",
        "            att_out_1 + att_out_2)\n",
        "        proj_output = self.dense_proj(att_out_2)\n",
        "        return self.normalization_layer_3(att_out_2 + proj_output)\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# If a saved model has to be loaded then use the below lines\n",
        "# Note for TA/ Professor: run this cell if loading a model for testing\n",
        "# Make sure to add correct path to model in model_path variable\n",
        "\n",
        "\n",
        "from keras.utils import custom_object_scope\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Define the custom objects dictionary\n",
        "custom_objects = {'PositionalEmbedding': PositionalEmbedding, 'TransformerEncoder': TransformerEncoder, 'TransformerDecoder': TransformerDecoder}\n",
        "\n",
        "with custom_object_scope(custom_objects):\n",
        "    model_path = \"/content/drive/MyDrive/Colab Notebooks/636 stuff/project2/Rahaan_Gandhi_434007427_Project2_Model.h5\" # path to model\n",
        "    loaded_model = models.load_model(model_path)\n",
        "\n",
        "# After running this cell, the model will be loaded into loaded_model and will be ready for use!"
      ],
      "metadata": {
        "id": "xb4_70O8su7r"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}